name: Benchmark

on:
  push:
    branches: [main]
    paths:
      - 'internal/**'
      - 'cmd/**'
      - 'benchmark/**'
      - 'go.mod'
      - 'go.sum'
  workflow_dispatch:
    inputs:
      questions:
        description: 'Number of questions to run (0 = all)'
        required: false
        default: '20'
      benchmark_type:
        description: 'Benchmark type'
        required: false
        default: 'locomo10'
        type: choice
        options:
          - locomo10
          - locomo_mc10
      random_sample:
        description: 'Use random sampling'
        required: false
        default: false
        type: boolean
      seed:
        description: 'Random seed (0 = random)'
        required: false
        default: '0'
  schedule:
    # Weekly full benchmark every Sunday at 3:00 AM UTC
    - cron: '0 3 * * 0'

jobs:
  # Quick benchmark on push and manual dispatch
  quick-benchmark:
    runs-on: ubuntu-latest
    if: github.event_name != 'schedule'
    timeout-minutes: 30

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-go@v5
        with:
          go-version: '1.21'
          cache: true

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: benchmark/locomo/requirements.txt

      - name: Install dependencies and build
        run: |
          pip install -r benchmark/locomo/requirements.txt &
          make build &
          wait
          python -c "import nltk; nltk.download('punkt', quiet=True); nltk.download('punkt_tab', quiet=True); nltk.download('wordnet', quiet=True)"

      - name: Download dataset
        run: |
          cd benchmark/locomo
          make download-fr
          make download-mc

      - name: Start server and run benchmark
        env:
          DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
        run: |
          ./mycelicmemory start --port 3099 --background
          sleep 3
          curl -sf http://localhost:3099/api/v1/health || (echo "Server failed to start" && exit 1)

          cd benchmark/locomo

          # Determine benchmark parameters
          BENCHMARK_TYPE="${{ github.event.inputs.benchmark_type || 'locomo10' }}"
          QUESTIONS="${{ github.event.inputs.questions || '20' }}"
          RANDOM_SAMPLE="${{ github.event.inputs.random_sample || 'false' }}"
          SEED="${{ github.event.inputs.seed || '0' }}"

          # Build command args
          EXTRA_ARGS=""
          if [ "$RANDOM_SAMPLE" = "true" ]; then
            EXTRA_ARGS="$EXTRA_ARGS --random-sample"
            if [ "$SEED" != "0" ]; then
              EXTRA_ARGS="$EXTRA_ARGS --seed $SEED"
            fi
          fi

          if [ "$BENCHMARK_TYPE" = "locomo10" ]; then
            python3 -m locomo10.main \
              --dataset data/locomo10.json \
              --output results/locomo10_results.json \
              --max-questions $QUESTIONS \
              --mycelicmemory-url http://localhost:3099/api/v1 \
              $EXTRA_ARGS
          else
            python3 -m locomo_mc10.main \
              --dataset data/locomo-mc10-full.jsonl \
              --output results/locomo_mc10_results.json \
              --max-questions $QUESTIONS \
              --mycelicmemory-url http://localhost:3099/api/v1 \
              $EXTRA_ARGS
          fi

      - uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results-quick
          path: benchmark/locomo/results/*.json
          retention-days: 30

      - name: Summary
        if: always()
        run: |
          BENCHMARK_TYPE="${{ github.event.inputs.benchmark_type || 'locomo10' }}"

          echo "## LoCoMo Benchmark Results ($BENCHMARK_TYPE)" >> $GITHUB_STEP_SUMMARY

          if [ "$BENCHMARK_TYPE" = "locomo10" ] && [ -f benchmark/locomo/results/locomo10_results.json ]; then
            python3 -c "
          import json
          with open('benchmark/locomo/results/locomo10_results.json') as f:
              data = json.load(f)
          results = data.get('results', {})
          total = len(results)
          if total > 0:
              f1_scores = [r.get('f1_score', 0) for r in results.values()]
              mean_f1 = sum(f1_scores) / len(f1_scores)
              print(f'- **Mean F1 Score**: {mean_f1:.3f}')
              print(f'- **Questions Evaluated**: {total}')
              by_cat = {}
              for r in results.values():
                  cat = r.get('category_name', 'unknown')
                  if cat not in by_cat:
                      by_cat[cat] = []
                  by_cat[cat].append(r.get('f1_score', 0))
              print('- **By Category**:')
              for cat, scores in sorted(by_cat.items()):
                  cat_f1 = sum(scores) / len(scores)
                  print(f'  - {cat}: {cat_f1:.3f} ({len(scores)} questions)')
          else:
              print('No results found')
          " >> $GITHUB_STEP_SUMMARY
          elif [ "$BENCHMARK_TYPE" = "locomo_mc10" ] && [ -f benchmark/locomo/results/locomo_mc10_results.json ]; then
            python3 -c "
          import json
          with open('benchmark/locomo/results/locomo_mc10_results.json') as f:
              data = json.load(f)
          results = data.get('results', {})
          total = len(results)
          if total > 0:
              correct = sum(1 for r in results.values() if r.get('is_correct', False))
              accuracy = correct / total * 100
              print(f'- **Accuracy**: {accuracy:.1f}%')
              print(f'- **Correct**: {correct}/{total}')
              by_type = {}
              for r in results.values():
                  qtype = r.get('question_type', 'unknown')
                  if qtype not in by_type:
                      by_type[qtype] = {'correct': 0, 'total': 0}
                  by_type[qtype]['total'] += 1
                  if r.get('is_correct', False):
                      by_type[qtype]['correct'] += 1
              print('- **By Question Type**:')
              for qtype, data in sorted(by_type.items()):
                  acc = data['correct'] / data['total'] * 100
                  print(f'  - {qtype}: {acc:.1f}% ({data[\"correct\"]}/{data[\"total\"]})')
          else:
              print('No results found')
          " >> $GITHUB_STEP_SUMMARY
          else
            echo "No results file found" >> $GITHUB_STEP_SUMMARY
          fi

  # Full benchmark on schedule (weekly)
  full-benchmark:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    timeout-minutes: 240  # 4 hours for full benchmark

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-go@v5
        with:
          go-version: '1.21'
          cache: true

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: benchmark/locomo/requirements.txt

      - name: Install dependencies and build
        run: |
          pip install -r benchmark/locomo/requirements.txt &
          make build &
          wait
          python -c "import nltk; nltk.download('punkt', quiet=True); nltk.download('punkt_tab', quiet=True); nltk.download('wordnet', quiet=True)"

      - name: Download dataset
        run: |
          cd benchmark/locomo
          make download-fr

      - name: Start server and run full benchmark
        env:
          DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
        run: |
          ./mycelicmemory start --port 3099 --background
          sleep 3
          curl -sf http://localhost:3099/api/v1/health || (echo "Server failed to start" && exit 1)

          cd benchmark/locomo

          # Run full benchmark with random sampling for variety
          python3 -m locomo10.main \
            --dataset data/locomo10.json \
            --output results/locomo10_full_results.json \
            --mycelicmemory-url http://localhost:3099/api/v1

      - uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results-full
          path: benchmark/locomo/results/*.json
          retention-days: 90

      - name: Summary
        if: always()
        run: |
          echo "## Full LoCoMo Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Scheduled weekly run (all 1986 questions)**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f benchmark/locomo/results/locomo10_full_results.json ]; then
            python3 -c "
          import json
          with open('benchmark/locomo/results/locomo10_full_results.json') as f:
              data = json.load(f)
          results = data.get('results', {})
          total = len(results)
          if total > 0:
              f1_scores = [r.get('f1_score', 0) for r in results.values()]
              mean_f1 = sum(f1_scores) / len(f1_scores)
              median_f1 = sorted(f1_scores)[len(f1_scores)//2]
              print(f'- **Mean F1 Score**: {mean_f1:.3f}')
              print(f'- **Median F1 Score**: {median_f1:.3f}')
              print(f'- **Questions Evaluated**: {total}')
              print(f'- **Total Time**: {data.get(\"total_time_seconds\", 0):.1f}s')
              print('')
              by_cat = {}
              for r in results.values():
                  cat = r.get('category_name', 'unknown')
                  if cat not in by_cat:
                      by_cat[cat] = []
                  by_cat[cat].append(r.get('f1_score', 0))
              print('### By Category')
              print('| Category | Mean F1 | Count |')
              print('|----------|---------|-------|')
              for cat, scores in sorted(by_cat.items()):
                  cat_f1 = sum(scores) / len(scores)
                  print(f'| {cat} | {cat_f1:.3f} | {len(scores)} |')
          else:
              print('No results found')
          " >> $GITHUB_STEP_SUMMARY
          else
            echo "No results file found" >> $GITHUB_STEP_SUMMARY
          fi
