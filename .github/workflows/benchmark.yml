name: Benchmark

on:
  push:
    branches: [main]
    paths:
      - 'internal/**'
      - 'cmd/**'
      - 'benchmark/**'
      - 'go.mod'
      - 'go.sum'
  workflow_dispatch:
    inputs:
      questions:
        description: 'Number of questions to run'
        required: false
        default: '20'

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-go@v5
        with:
          go-version: '1.21'
          cache: true

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: benchmark/locomo/requirements.txt

      - name: Install dependencies and build
        run: |
          pip install -r benchmark/locomo/requirements.txt &
          make build &
          wait
          python -c "import nltk; nltk.download('punkt', quiet=True); nltk.download('punkt_tab', quiet=True); nltk.download('wordnet', quiet=True)"

      - name: Download dataset
        run: |
          cd benchmark/locomo
          make download-fr

      - name: Start server and run benchmark
        env:
          DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
        run: |
          ./ultrathink start --port 3099 --background
          sleep 3
          curl -sf http://localhost:3099/api/v1/health || (echo "Server failed to start" && exit 1)

          cd benchmark/locomo
          python3 -m locomo10.main \
            --dataset data/locomo10.json \
            --output results/locomo10_results.json \
            --max-questions ${{ github.event.inputs.questions || '20' }} \
            --ultrathink-url http://localhost:3099/api/v1

      - uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results
          path: benchmark/locomo/results/*.json
          retention-days: 30

      - name: Summary
        if: always()
        run: |
          echo "## LoCoMo Free-Response Benchmark Results" >> $GITHUB_STEP_SUMMARY
          if [ -f benchmark/locomo/results/locomo10_results.json ]; then
            python3 -c "
          import json
          with open('benchmark/locomo/results/locomo10_results.json') as f:
              data = json.load(f)
          results = data.get('results', {})
          total = len(results)
          if total > 0:
              f1_scores = [r.get('f1_score', 0) for r in results.values()]
              mean_f1 = sum(f1_scores) / len(f1_scores)
              print(f'- **Mean F1 Score**: {mean_f1:.3f}')
              print(f'- **Questions Evaluated**: {total}')
              # Per-category breakdown
              by_cat = {}
              for r in results.values():
                  cat = r.get('category_name', 'unknown')
                  if cat not in by_cat:
                      by_cat[cat] = []
                  by_cat[cat].append(r.get('f1_score', 0))
              print('- **By Category**:')
              for cat, scores in sorted(by_cat.items()):
                  cat_f1 = sum(scores) / len(scores)
                  print(f'  - {cat}: {cat_f1:.3f} ({len(scores)} questions)')
          else:
              print('No results found')
          " >> $GITHUB_STEP_SUMMARY
          else
            echo "No results file found" >> $GITHUB_STEP_SUMMARY
          fi
